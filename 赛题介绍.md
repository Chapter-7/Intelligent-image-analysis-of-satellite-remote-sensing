# 赛题介绍

卫星图像的普及从根本上提高了我们对地球的认识。它使我们能够更好地实现从灾难发生时调动资源到监测全球变暖的影响等一切目标。人们往往认为理所当然的是，这些进步都依赖于完全手工或通过不完善的半自动化方法标注建筑足迹和道路等重要特征。

随着这些大型复杂数据集的数量呈指数级增长，我们正在寻求新颖的解决方案，以减轻其图像分析人员的负担。在这项比赛中，参赛者面临的挑战是对俯瞰图像中的特征进行准确分类。实现特征标注自动化不仅能帮助我们围绕国防和安全问题更快地做出明智决策，还能为应用于卫星图像的计算机视觉方法带来创新。

### **评估**

我们将根据预测的多聚点与实际多聚点之间的**平均 Jaccard 指数**对提交的数据进行评估。这是一个基于向量的指标，我们使用多边形几何图形来评估您的预测与答案的吻合程度。

两个区域 A 和 B 的[Jaccard 指数](https://en.wikipedia.org/wiki/Jaccard_index)（也称为 “交集大于联合”）定义为
$$
 J a c c a r d = \frac { T P } { T P + F P + F N } = \frac { | A \cap B | } { | A \cup B | } = \frac { | A \cap B | } { | A | + | B | - | A \cap B | }
$$
其中，TP 为真阳性区域，FP 为假阳性区域，FN 为假阴性区域。

对于每张图像的每个对象类别，我们都要计算出 TP、FP 和 FN 区域。然后，我们将所有图像的总 TP、总 FP 和总 FN 相加，再利用总 TP、总 FP 和总 FN 计算出该类别的 Jaccard 值。然后，我们求出所有 10 个类别的 Jaccard 指数的平均值。

## **提交文件** 

对于提交文件应该包含第一章的需求分析，操作说明等。如果对模型进行调优，剪枝等操作需要进行撰写文档进行说明。

## 技术

数据集中涉及WKT格式，可以选择使用python的shapely包的loads读取

读取geojson格式Python - [json](https://docs.python.org/2/library/json.html) 、 [geojson](https://pypi.python.org/pypi/geojson) 、 [shapely](http://toblerity.org/shapely/) （使用 json，然后转换为 shapely 的`MultiPolygon` ）

### 如何将几何图形投影到像素坐标

在我们提供的这个数据集中，我们创建了一组位于 x = [0,1] 和 y = [-1,0] 范围内的地理坐标。这些坐标经过转换，使我们模糊了卫星图像的拍摄位置。这些图像来自地球上的同一地区。

为了利用这些图像，我们提供每个图像的网格坐标，以便您知道如何缩放它们并将它们与图像（以像素为单位）对齐。在 grid_sizes.csv 中，您将获得每个 imageId 的 Xmax 和 Ymin 值。

对于每个图像，您应该能够从图像光栅中获取宽度 (W) 和高度 (H)。对于 3391 x 3349 x 3 的 3 波段图像，W 为 3349，H 为 3391。然后您可以按如下方式缩放数据：
$$
 W ^ { \prime } = W \cdot \frac { W } { W + 1 }
$$

$$
x ' = \frac { x } { x _ { m a x } } \cdot W '
$$

$$
H ' = H \cdot \frac { H } { H + 1 }
$$

$$
 y ' = \frac { y } { y _ { \min } } \cdot H '
$$

### 如何将结果转换回多边形

最好使用cascaded_union() 将多边形列表展平为不重叠的多边形。

将结果缩放回原始坐标：
$$
x = x ^ { \prime } \cdot \frac { x _ { m a x } } { W ^ { \prime } }
$$

$$
y = y ^ { \prime } \cdot \frac { y \min } { H ^ { \prime } }
$$

然后很容易使用[shapely](http://toblerity.org/shapely/)将多边形输出为wkt格式： `multipoly.wkt`

### 如何在 Python 中打开 GeoTiff 文件

[GDAL](http://www.gdal.org/)很棒，但安装有点困难。如果您只想将文件作为栅格读取， [tifffile](https://pypi.python.org/pypi/tifffile)是一个用于打开 GeoTiff 文件的轻量级软件包。

或者你可以使用`tifffile`

## 模型介绍

### Unet介绍

UNet模型是一种基于卷积[神经网络](https://cloud.baidu.com/product/wenxinworkshop)的图像分割算法，它采用了U型的网络结构，由编码器（下采样路径）和解码器（上采样路径）两部分组成。编码器负责提取输入图像的特征，而解码器则通过上采样操作将特征图恢复到原始输入图像的尺寸，并逐步生成分割结果。UNet的关键创新在于解码器中引入了跳跃连接（Skip Connections），将编码器中的特征图与解码器中对应的特征图进行连接。这种跳跃连接有助于解码器更好地利用不同层次的特征信息，从而提高图像分割的准确性和细节保留能力。

UNet算法在医学图像分割领域表现出色，特别适用于小样本、不平衡数据和需要保留细节信息的任务。例如，在肿瘤检测、血管分割、视网膜病变识别等方面，UNet模型都取得了显著的成果。此外，UNet模型还可以应用于其他领域的图像分割任务，如卫星遥感图像分割、自动驾驶中的语义分割等。

在实际应用中，使用UNet模型进行图像分割时需要注意以下几点：

1. 数据预处理：为了提高模型的分割效果，需要对输入图像进行适当的预处理，如灰度化、归一化、去噪等。
2. 参数调整：根据具体任务和数据集的特点，需要调整模型的参数，如学习率、批次大小、迭代次数等。此外，还可以尝试使用不同的优化器和损失函数来优化模型的性能。
3. 后处理：为了提高分割结果的准确性，可以在模型输出后进行一些后处理操作，如形态学操作、条件滤波等。
4. 模型扩展与改进：针对具体任务和数据集的特点，可以尝试对UNet模型进行扩展和改进，如引入注意力机制、多尺度特征融合等。

### Deeplabv模型

连续的池化和下采样，使特征分辨率下降，不利于定位
全局特征或上下文之间的互相作用有利于语义分割的效果
 
deeplabv3的主要贡献

提出了更通用的框架，适用于更多网络
改进了ASPP：由不同采样率的空洞卷、BN层组成，尝试以级联并行的方式设计模块
大采样的空洞卷积：使用大采样率的3x3 的空洞卷积，此时由于图像边界响应无法捕捉远距离信号，就会退化成1x1的卷积

全局特征或上下文之间的互相作用有助于做语义分割。下图是四种不同类型的做语义分割的全卷积神经网络：

<img src="C:\Users\EDY\Desktop\新建文件夹\文件\img/2.png">

#### 图像金字塔（image pyramid）

将输入图片放缩成不同的比例，分别应用在DCNN上。
使用共享权重模型，小尺寸的输入响应控制语义、大尺度的输入响应控制细节。
将预测结果融合得到最终输出
缺点：因为GPU存储器的限制，对于更大更深的模型不方便扩展，通常应用于推断阶段

#### 编码-解码（encoder-decoder）

【编码器】将信息编码为压缩向量来代表输入；【解码器】：将这个信号重建为期望的输出
好处：编码器的高层次的特征容易捕获更长的距离信息，在解码器阶段使用编码器阶段的信息 帮助恢复目标的细节和空间维度
例子：SegNet 利用下采样的池化索引作为上采样的指导；U-Net 增加了编码器部分的特征跳跃 连接到解码器；RefineNet 等证明了Encoder-Decoder 结构的有效性

#### 上下文模块（Context module）：

在原始模型的顶端增加额外的模块（例如 DenseCRF），来捕捉像素间长距离信息

#### 空间金字塔池化（Spatial pyramid pooling）

采用空间金字塔池化，可以捕捉多个层次的上下文
在 ParseNet 中，从不同图像等级的特征中捕获上下文信息；deeplabv2 提出了ASPP，以不同采样率的并行空洞卷积来捕获多尺度信息。PSPNet 在不同网络尺度上执行空间池化，并在多个数据集上具有优异的表现

### deeplabv3的结构

deeplab提出了一种新颖的语义分割方法：控制特征的抽取、学习多尺度特征的网络结构。

deeplabv3 的特征提取模块：在ImageNet 上预训练的 ResNet
注意最后一个 ResNet Block：
使用了空洞卷积。这个残差块内的卷积都是使用了不同的rate 来捕获多尺度信息
顶部使用了空洞空间金字塔池化（ASPP）

DeepLabv3基于卷积[神经网络](https://cloud.baidu.com/product/wenxinworkshop)（CNN）构建，采用了ASPP（Atrous Spatial Pyramid Pooling）模块和decoder来提升对不同尺度目标的识别能力。ASPP通过在卷积后的特征图上应用不同 atrous rate 的卷积核，提取不同尺度的特征。decoder则将这些特征图解码为与原图大小相同的分割图。

DeepLabv3主要由卷积层、ASPP模块和decoder组成。卷积层用于提取图像特征，ASPP模块用于提取不同尺度的特征，而decoder则将特征图解码为分割图。整个网络结构如下：

1. 卷积层：使用多个卷积层对输入图像进行特征提取。常用的卷积层包括3x3卷积、1x1卷积等。
2. ASPP模块：在卷积后的特征图上应用不同 atrous rate 的卷积核，提取不同尺度的特征。通过在ASPP中设置不同的 atrous rate，可以提取不同尺度大小的特征。
3. Decoder：将ASPP输出的特征图进行上采样，并与相应尺度的特征图进行拼接，得到与原图大小相同的分割图。

要在PyTorch中实现DeepLabv3的推理过程，首先需要加载预训练模型。可以使用PyTorch提供的torchvision.models模块加载预训练的DeepLabv3模型。加载模型后，可以将需要预测的图像数据输入到模型中进行推理。推理过程如下：

1. 加载预训练模型：使用torchvision.models模块加载预训练的DeepLabv3模型。例如：`model = torchvision.models.segmentation.deeplabv3_resnet101(pretrained=True)`
2. 准备输入数据：将需要预测的图像数据转换为模型所需的输入格式。通常需要将图像数据归一化并转换为模型所需的输入尺寸。例如：`input_tensor = transforms.Compose([transforms.ToTensor(), transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])])(image)`
3. 进行推理：将输入数据输入到模型中，即可得到预测结果。例如：`output = model(input_tensor)`
4. 处理输出结果：将模型的输出结果解码为分割图。可以使用PyTorch提供的decode方法将模型的输出解码为分割图。例如：`predictions = model(input_tensor).argmax(dim=0)`
5. 可视化结果：将分割图可视化，以便更好地理解预测结果。可以使用matplotlib等库进行可视化。例如：`plt.imshow(predictions[0].cpu().numpy(), cmap='gray')`

通过以上步骤，您可以在PyTorch中使用DeepLabv3进行图像分割任务的。

### deeplabv3+

DeeplabV3+模型基于Encoder-Decoder架构，通过ASPP（Atrous Spatial Pyramid Pooling）模块增强Encoder的语义信息提取能力，并通过Decoder实现像素级的预测。ASPP模块通过在不同空洞卷积的感受野中提取特征，增强了模型对不同尺度目标的感知能力。同时，DeeplabV3+采用了多尺度预测的策略，提高了模型的鲁棒性。

DeeplabV3+模型主要由Encoder、ASPP模块和Decoder三部分组成。Encoder采用ResNet-101[网络](https://cloud.baidu.com/product/et.html)结构，通过残差连接和瓶颈结构增强特征提取能力。ASPP模块包含多个不同空洞率的卷积层，能够提取不同尺度特征。Decoder采用上采样和卷积操作，将高层次的语义信息逐步融合到低层次的特征中，实现像素级的预测。



DeepLabv3+是在DeepLabv3的基础上进行改进的模型，主要增加了encoder-decoder结构，提高了对图像中细节信息的分割能力。在DeepLabv3+中，使用了ASPP模块与encoder-decoder结构相结合的方式，使得模型能够更好地捕获图像中的上下文信息和细节信息。此外，DeepLabv3+还采用了类似于U-Net的跳跃连接方式，将编码器与解码器连接起来，使得特征信息能够在不同的层级之间传递，进一步提高了模型的分割精度。



在性能方面，DeepLabv3+相较于DeepLabv3在语义分割任务上取得了更好的效果。这主要得益于encoder-decoder结构和跳跃连接的应用，使得模型能够更好地捕获图像中的上下文信息和细节信息。然而，DeepLabv3+的计算量相对较大，可能会增加模型的训练时间和内存开销。



使用Deeplab V3+(Xception 和mobilenet V2 backbone)



### **1. Xception网络简介**

Xception (Extreme Inception) 是 Google 提出的改进版的 Inception 模型，主要创新在于：

- 使用 **深度可分离卷积** 替代标准卷积操作。
- 强调特征通道之间的解耦，减少冗余特征计算。

#### **深度可分离卷积**

深度可分离卷积分为两部分：

1. **Depthwise Convolution**：对每个输入通道单独执行卷积操作。
2. **Pointwise Convolution**：通过 1×1 卷积合并所有通道的信息。

这种操作显著降低了计算成本和参数量，同时保留了特征学习的能力。

#### **Xception的特点**

- 更深的网络结构。
- 更少的参数和计算量。
- 优化特征提取效率。

------

### **2. DeepLabV3+中的Xception改进**

DeepLabV3+ 使用了一个改进版的 Xception 作为其主干网络，以增强对空间特征和上下文信息的捕获。改进的主要点包括：

#### **(1) 深度可分离卷积的引入**

DeepLabV3+ 在主干网络和 ASPP（Atrous Spatial Pyramid Pooling）模块中，全面采用深度可分离卷积，不仅降低了计算成本，还提升了特征提取的效率。

#### **(2) Atrous Convolution 支持多尺度特征提取**

在 Xception 的卷积层中，部分卷积替换为 **空洞卷积（Atrous Convolution）**，能够有效扩大感受野，从而在不增加计算量的情况下捕获多尺度上下文信息。

#### **(3) 改进的下采样和上采样策略**

- **下采样**：通过逐步减少分辨率，提取高效的多尺度特征。
- **上采样**：利用解码器对高分辨率特征进行恢复，增强对边界的捕获能力。

#### **(4) 修改后的残差连接**

DeepLabV3+ 中的 Xception 结合残差结构，进一步增强了网络的梯度传播能力和稳定性。

------

### **3. Xception在DeepLabV3+中的流程**

1. **初始卷积和最大池化**：前几层使用标准卷积捕获基础特征。
2. **深度可分离卷积块**：利用深度可分离卷积提取高级语义特征。
3. **空洞卷积增强上下文**：在特定卷积块中，使用空洞卷积扩大感受野。
4. **输出多尺度特征**：结合 ASPP 模块进一步处理，提供多尺度上下文信息。

------

### **4. 优势与局限性**

#### **优势**

- **高效性**：深度可分离卷积减少了计算量和参数量。
- **多尺度上下文**：结合空洞卷积和 ASPP 提供更丰富的上下文特征。
- **边缘细节保留**：解码器能够很好地恢复分辨率，提高分割结果的边界质量。

#### **局限性**

- 对显存需求较高，尤其在处理高分辨率图像时。
- 依赖良好的超参数调节以达到最佳性能。



### **1. MobileNetV2 网络简介**

#### **核心创新：反向残差块（Inverted Residual Block）**

MobileNetV2 引入了 **反向残差结构** 和 **线性瓶颈结构**，其主要特点是：

1. **反向残差块**：输入和输出的通道数较少（瓶颈层），中间通道数较多（扩展层）。这种设计减少了内存占用并提高了计算效率。
2. 深度可分离卷积
   - **Depthwise Convolution**：对每个输入通道单独进行卷积。
   - **Pointwise Convolution**：通过 1×1 卷积将特征通道进行组合。
3. **线性激活**：在瓶颈层不使用非线性激活函数（ReLU），保留更多的特征信息。

这些改进使 MobileNetV2 的计算量和参数量大幅降低，同时保证了模型的表达能力。

------

### **2. MobileNetV2 在 DeepLabV3+ 中的应用**

在 DeepLabV3+ 中，MobileNetV2 作为主干网络（backbone），经过优化和改进，用于提取高效的特征表示，尤其是在计算资源有限的情况下表现优异。

#### **(1) 主干网络的层次设计**

MobileNetV2 的网络分为多个阶段，每个阶段由多个反向残差块堆叠组成，输出逐步缩小分辨率的特征图：

- 第一阶段：低级特征提取（高分辨率，低语义信息）。
- 中间阶段：深度特征提取（中分辨率，中语义信息）。
- 后期阶段：高级语义特征提取（低分辨率，高语义信息）。

#### **(2) 结合空洞卷积（Atrous Convolution）**

在 DeepLabV3+ 中，MobileNetV2 被修改以适应语义分割任务：

- 在部分层中引入空洞卷积，用于扩大感受野，同时保持分辨率不降低。
- 空洞卷积的多尺度组合（如 ASPP 模块）可以提取丰富的上下文信息。

#### **(3) 支持解码器模块**

DeepLabV3+ 在 MobileNetV2 backbone 的基础上，使用解码器模块恢复高分辨率特征图，提升边界细节的分割效果。

------

### **3. MobileNetV2 在语义分割中的优点**

#### **(1) 计算效率高**

得益于深度可分离卷积和反向残差结构，MobileNetV2 的参数量和计算量远小于传统卷积神经网络（如 ResNet、Xception）。

#### **(2) 易于在移动设备上部署**

MobileNetV2 设计轻量，适合资源有限的设备，在边缘计算场景中应用广泛。

#### **(3) 良好的特征表达能力**

尽管网络轻量化，MobileNetV2 仍然可以提供足够的语义特征，用于高质量的语义分割任务。

#### **(4) 与解码器配合良好**

DeepLabV3+ 的解码器模块弥补了轻量化模型高分辨率特征不足的缺陷，提高了分割边界的精确度。

------

### **4. MobileNetV2 的局限性**

1. 模型精度略低
   - 相较于更复杂的主干网络（如 Xception、ResNet），MobileNetV2 的特征提取能力略逊一筹。
2. 适合中等复杂度场景
   - 在非常复杂的场景下，可能需要更高分辨率的特征提取。

## 赛题环境

比赛开始前统一分发赛题任务书、比赛文件，竞赛平台已内置好开发工具和环境，参赛选手基于现有的资源进行竞赛

## 评分方式

采用“脚本”进行自动评分，后续进行人工核对。每一个任务每一题都设计成具有唯一解法，且结果可验证

## 主要问题

过拟合和欠拟合：
训练轮数过少：可能导致模型欠拟合，即模型未能充分学习数据的特征，导致在训练集和测试集上表现都不好。

训练轮数过多：可能导致模型过拟合，即模型在训练集上表现很好，但在测试集上表现不佳。此时，模型过于记忆训练数据中的噪声和细节，缺乏泛化能力